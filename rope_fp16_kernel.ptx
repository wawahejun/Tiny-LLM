//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30672275
// Cuda compilation tools, release 11.5, V11.5.119
// Based on NVVM 7.0.1
//

.version 7.5
.target sm_86
.address_size 64

	// .globl	rope_fp16
.global .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry rope_fp16(
	.param .u64 rope_fp16_param_0,
	.param .u32 rope_fp16_param_1,
	.param .f32 rope_fp16_param_2,
	.param .u32 rope_fp16_param_3,
	.param .u32 rope_fp16_param_4,
	.param .u32 rope_fp16_param_5
)
{
	.local .align 4 .b8 	__local_depot0[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<43>;
	.reg .f32 	%f<155>;
	.reg .b32 	%r<100>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<26>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd9, [rope_fp16_param_0];
	ld.param.u32 	%r21, [rope_fp16_param_1];
	ld.param.f32 	%f23, [rope_fp16_param_2];
	ld.param.u32 	%r24, [rope_fp16_param_3];
	ld.param.u32 	%r22, [rope_fp16_param_4];
	ld.param.u32 	%r23, [rope_fp16_param_5];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p2, %r1, %r24;
	mov.u32 	%r2, %ctaid.y;
	setp.ge.s32 	%p3, %r2, %r22;
	or.pred  	%p4, %p2, %p3;
	shr.u32 	%r25, %r23, 31;
	add.s32 	%r26, %r23, %r25;
	shr.s32 	%r27, %r26, 1;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p5, %r3, %r27;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB0_24;

	cvta.to.global.u64 	%rd2, %rd9;
	add.s32 	%r28, %r1, %r21;
	cvt.rn.f32.s32 	%f1, %r28;
	shl.b32 	%r29, %r3, 1;
	cvt.rn.f32.s32 	%f25, %r29;
	cvt.rn.f32.s32 	%f26, %r23;
	div.rn.f32 	%f2, %f25, %f26;
	mul.f32 	%f27, %f2, 0f3F000000;
	cvt.rzi.f32.f32 	%f28, %f27;
	add.f32 	%f29, %f28, %f28;
	sub.f32 	%f30, %f2, %f29;
	abs.f32 	%f3, %f30;
	abs.f32 	%f4, %f23;
	setp.lt.f32 	%p7, %f4, 0f00800000;
	mul.f32 	%f31, %f4, 0f4B800000;
	selp.f32 	%f32, %f31, %f4, %p7;
	selp.f32 	%f33, 0fC3170000, 0fC2FE0000, %p7;
	mov.b32 	%r30, %f32;
	and.b32  	%r31, %r30, 8388607;
	or.b32  	%r32, %r31, 1065353216;
	mov.b32 	%f34, %r32;
	shr.u32 	%r33, %r30, 23;
	cvt.rn.f32.u32 	%f35, %r33;
	add.f32 	%f36, %f33, %f35;
	setp.gt.f32 	%p8, %f34, 0f3FB504F3;
	mul.f32 	%f37, %f34, 0f3F000000;
	add.f32 	%f38, %f36, 0f3F800000;
	selp.f32 	%f39, %f38, %f36, %p8;
	selp.f32 	%f40, %f37, %f34, %p8;
	add.f32 	%f41, %f40, 0fBF800000;
	add.f32 	%f42, %f40, 0f3F800000;
	rcp.approx.ftz.f32 	%f43, %f42;
	add.f32 	%f44, %f41, %f41;
	mul.f32 	%f45, %f44, %f43;
	mul.f32 	%f46, %f45, %f45;
	mov.f32 	%f47, 0f3C4CAF63;
	mov.f32 	%f48, 0f3B18F0FE;
	fma.rn.f32 	%f49, %f48, %f46, %f47;
	mov.f32 	%f50, 0f3DAAAABD;
	fma.rn.f32 	%f51, %f49, %f46, %f50;
	mul.rn.f32 	%f52, %f51, %f46;
	mul.rn.f32 	%f53, %f52, %f45;
	sub.f32 	%f54, %f41, %f45;
	add.f32 	%f55, %f54, %f54;
	neg.f32 	%f56, %f45;
	fma.rn.f32 	%f57, %f56, %f41, %f55;
	mul.rn.f32 	%f58, %f43, %f57;
	add.f32 	%f59, %f53, %f45;
	sub.f32 	%f60, %f45, %f59;
	add.f32 	%f61, %f53, %f60;
	add.f32 	%f62, %f58, %f61;
	add.f32 	%f63, %f59, %f62;
	sub.f32 	%f64, %f59, %f63;
	add.f32 	%f65, %f62, %f64;
	mov.f32 	%f66, 0f3F317200;
	mul.rn.f32 	%f67, %f39, %f66;
	mov.f32 	%f68, 0f35BFBE8E;
	mul.rn.f32 	%f69, %f39, %f68;
	add.f32 	%f70, %f67, %f63;
	sub.f32 	%f71, %f67, %f70;
	add.f32 	%f72, %f63, %f71;
	add.f32 	%f73, %f65, %f72;
	add.f32 	%f74, %f69, %f73;
	add.f32 	%f75, %f70, %f74;
	sub.f32 	%f76, %f70, %f75;
	add.f32 	%f77, %f74, %f76;
	abs.f32 	%f5, %f2;
	setp.gt.f32 	%p9, %f5, 0f77F684DF;
	mul.f32 	%f78, %f2, 0f39000000;
	selp.f32 	%f79, %f78, %f2, %p9;
	mul.rn.f32 	%f80, %f79, %f75;
	neg.f32 	%f81, %f80;
	fma.rn.f32 	%f82, %f79, %f75, %f81;
	fma.rn.f32 	%f83, %f79, %f77, %f82;
	mov.f32 	%f84, 0f00000000;
	fma.rn.f32 	%f85, %f84, %f75, %f83;
	add.rn.f32 	%f86, %f80, %f85;
	neg.f32 	%f87, %f86;
	add.rn.f32 	%f88, %f80, %f87;
	add.rn.f32 	%f89, %f88, %f85;
	mov.b32 	%r34, %f86;
	setp.eq.s32 	%p10, %r34, 1118925336;
	add.s32 	%r35, %r34, -1;
	mov.b32 	%f90, %r35;
	add.f32 	%f91, %f89, 0f37000000;
	selp.f32 	%f6, %f91, %f89, %p10;
	selp.f32 	%f92, %f90, %f86, %p10;
	mov.f32 	%f93, 0f3FB8AA3B;
	mul.rn.f32 	%f94, %f92, %f93;
	cvt.rzi.f32.f32 	%f95, %f94;
	abs.f32 	%f96, %f95;
	setp.gt.f32 	%p11, %f96, 0f42FC0000;
	mov.b32 	%r36, %f95;
	and.b32  	%r37, %r36, -2147483648;
	or.b32  	%r38, %r37, 1123811328;
	mov.b32 	%f97, %r38;
	selp.f32 	%f98, %f97, %f95, %p11;
	mov.f32 	%f99, 0fBF317218;
	fma.rn.f32 	%f100, %f98, %f99, %f92;
	mov.f32 	%f101, 0f3102E308;
	fma.rn.f32 	%f102, %f98, %f101, %f100;
	mul.f32 	%f103, %f102, 0f3FB8AA3B;
	add.f32 	%f104, %f98, 0f4B40007F;
	mov.b32 	%r39, %f104;
	shl.b32 	%r40, %r39, 23;
	mov.b32 	%f105, %r40;
	ex2.approx.ftz.f32 	%f106, %f103;
	mul.f32 	%f7, %f106, %f105;
	setp.eq.f32 	%p12, %f7, 0f7F800000;
	mov.f32 	%f151, 0f7F800000;
	@%p12 bra 	$L__BB0_3;

	fma.rn.f32 	%f151, %f7, %f6, %f7;

$L__BB0_3:
	setp.lt.f32 	%p13, %f23, 0f00000000;
	setp.eq.f32 	%p14, %f3, 0f3F800000;
	and.pred  	%p1, %p13, %p14;
	setp.eq.f32 	%p15, %f23, 0f00000000;
	@%p15 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_4;

$L__BB0_7:
	add.f32 	%f110, %f23, %f23;
	mov.b32 	%r43, %f110;
	selp.b32 	%r44, %r43, 0, %p14;
	or.b32  	%r45, %r44, 2139095040;
	setp.lt.f32 	%p19, %f2, 0f00000000;
	selp.b32 	%r46, %r45, %r44, %p19;
	mov.b32 	%f153, %r46;
	bra.uni 	$L__BB0_8;

$L__BB0_4:
	mov.b32 	%r41, %f151;
	xor.b32  	%r42, %r41, -2147483648;
	mov.b32 	%f107, %r42;
	selp.f32 	%f153, %f107, %f151, %p1;
	setp.geu.f32 	%p16, %f23, 0f00000000;
	@%p16 bra 	$L__BB0_8;

	cvt.rzi.f32.f32 	%f108, %f2;
	setp.eq.f32 	%p17, %f108, %f2;
	@%p17 bra 	$L__BB0_8;

	mov.f32 	%f153, 0f7FFFFFFF;

$L__BB0_8:
	add.f32 	%f111, %f4, %f5;
	mov.b32 	%r47, %f111;
	setp.lt.s32 	%p20, %r47, 2139095040;
	@%p20 bra 	$L__BB0_15;

	setp.gtu.f32 	%p21, %f4, 0f7F800000;
	setp.gtu.f32 	%p22, %f5, 0f7F800000;
	or.pred  	%p23, %p21, %p22;
	@%p23 bra 	$L__BB0_14;
	bra.uni 	$L__BB0_10;

$L__BB0_14:
	add.f32 	%f153, %f2, %f23;
	bra.uni 	$L__BB0_15;

$L__BB0_10:
	setp.eq.f32 	%p24, %f5, 0f7F800000;
	@%p24 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_11;

$L__BB0_13:
	setp.gt.f32 	%p27, %f4, 0f3F800000;
	selp.b32 	%r51, 2139095040, 0, %p27;
	xor.b32  	%r52, %r51, 2139095040;
	setp.lt.f32 	%p28, %f2, 0f00000000;
	selp.b32 	%r53, %r52, %r51, %p28;
	mov.b32 	%f112, %r53;
	setp.eq.f32 	%p29, %f23, 0fBF800000;
	selp.f32 	%f153, 0f3F800000, %f112, %p29;
	bra.uni 	$L__BB0_15;

$L__BB0_11:
	setp.neu.f32 	%p25, %f4, 0f7F800000;
	@%p25 bra 	$L__BB0_15;

	setp.ge.f32 	%p26, %f2, 0f00000000;
	selp.b32 	%r48, 2139095040, 0, %p26;
	or.b32  	%r49, %r48, -2147483648;
	selp.b32 	%r50, %r49, %r48, %p1;
	mov.b32 	%f153, %r50;

$L__BB0_15:
	setp.eq.f32 	%p30, %f2, 0f00000000;
	setp.eq.f32 	%p31, %f23, 0f3F800000;
	or.pred  	%p32, %p31, %p30;
	selp.f32 	%f113, 0f3F800000, %f153, %p32;
	div.rn.f32 	%f17, %f1, %f113;
	mul.f32 	%f114, %f17, 0f3F22F983;
	cvt.rni.s32.f32 	%r99, %f114;
	cvt.rn.f32.s32 	%f115, %r99;
	mov.f32 	%f116, 0fBFC90FDA;
	fma.rn.f32 	%f117, %f115, %f116, %f17;
	mov.f32 	%f118, 0fB3A22168;
	fma.rn.f32 	%f119, %f115, %f118, %f117;
	mov.f32 	%f120, 0fA7C234C5;
	fma.rn.f32 	%f154, %f115, %f120, %f119;
	abs.f32 	%f19, %f17;
	setp.leu.f32 	%p33, %f19, 0f47CE4780;
	@%p33 bra 	$L__BB0_23;

	setp.eq.f32 	%p34, %f19, 0f7F800000;
	@%p34 bra 	$L__BB0_22;
	bra.uni 	$L__BB0_17;

$L__BB0_22:
	mov.f32 	%f150, 0f00000000;
	mul.rn.f32 	%f154, %f17, %f150;
	bra.uni 	$L__BB0_23;

$L__BB0_17:
	mov.b32 	%r5, %f17;
	bfe.u32 	%r55, %r5, 23, 8;
	add.s32 	%r6, %r55, -128;
	shl.b32 	%r56, %r5, 8;
	or.b32  	%r7, %r56, -2147483648;
	shr.u32 	%r8, %r6, 5;
	mov.u64 	%rd25, 0;
	mov.u32 	%r96, 0;
	mov.u64 	%rd23, __cudart_i2opi_f;
	mov.u64 	%rd24, %rd1;

$L__BB0_18:
	.pragma "nounroll";
	ld.global.nc.u32 	%r57, [%rd23];
	mad.wide.u32 	%rd13, %r57, %r7, %rd25;
	shr.u64 	%rd25, %rd13, 32;
	st.local.u32 	[%rd24], %rd13;
	add.s64 	%rd24, %rd24, 4;
	add.s64 	%rd23, %rd23, 4;
	add.s32 	%r96, %r96, 1;
	setp.ne.s32 	%p35, %r96, 6;
	@%p35 bra 	$L__BB0_18;

	st.local.u32 	[%rd1+24], %rd25;
	mov.u32 	%r58, 4;
	sub.s32 	%r11, %r58, %r8;
	mov.u32 	%r59, 6;
	sub.s32 	%r60, %r59, %r8;
	mul.wide.s32 	%rd14, %r60, 4;
	add.s64 	%rd15, %rd1, %rd14;
	ld.local.u32 	%r97, [%rd15];
	ld.local.u32 	%r98, [%rd15+-4];
	and.b32  	%r14, %r6, 31;
	setp.eq.s32 	%p36, %r14, 0;
	@%p36 bra 	$L__BB0_21;

	mov.u32 	%r61, 32;
	sub.s32 	%r62, %r61, %r14;
	shr.u32 	%r63, %r98, %r62;
	shl.b32 	%r64, %r97, %r14;
	add.s32 	%r97, %r63, %r64;
	mul.wide.s32 	%rd16, %r11, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ld.local.u32 	%r65, [%rd17];
	shr.u32 	%r66, %r65, %r62;
	shl.b32 	%r67, %r98, %r14;
	add.s32 	%r98, %r66, %r67;

$L__BB0_21:
	and.b32  	%r68, %r5, -2147483648;
	shr.u32 	%r69, %r98, 30;
	shl.b32 	%r70, %r97, 2;
	or.b32  	%r71, %r69, %r70;
	shr.u32 	%r72, %r71, 31;
	shr.u32 	%r73, %r97, 30;
	add.s32 	%r74, %r72, %r73;
	neg.s32 	%r75, %r74;
	setp.eq.s32 	%p37, %r68, 0;
	selp.b32 	%r99, %r74, %r75, %p37;
	setp.ne.s32 	%p38, %r72, 0;
	xor.b32  	%r76, %r68, -2147483648;
	selp.b32 	%r77, %r76, %r68, %p38;
	selp.b32 	%r78, -1, 0, %p38;
	xor.b32  	%r79, %r71, %r78;
	shl.b32 	%r80, %r98, 2;
	xor.b32  	%r81, %r80, %r78;
	cvt.u64.u32 	%rd18, %r79;
	cvt.u64.u32 	%rd19, %r81;
	bfi.b64 	%rd20, %rd18, %rd19, 32, 32;
	cvt.rn.f64.s64 	%fd1, %rd20;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f121, %fd2;
	setp.eq.s32 	%p39, %r77, 0;
	neg.f32 	%f122, %f121;
	selp.f32 	%f154, %f121, %f122, %p39;

$L__BB0_23:
	mov.u32 	%r95, %ctaid.y;
	ld.param.u32 	%r94, [rope_fp16_param_4];
	mov.f32 	%f149, 0f00000000;
	ld.param.u32 	%r93, [rope_fp16_param_5];
	mov.u32 	%r92, %ctaid.x;
	mul.f32 	%f126, %f154, %f154;
	mov.f32 	%f127, 0fBAB607ED;
	mov.f32 	%f128, 0f37CBAC00;
	fma.rn.f32 	%f129, %f128, %f126, %f127;
	mov.f32 	%f130, 0f3D2AAABB;
	fma.rn.f32 	%f131, %f129, %f126, %f130;
	mov.f32 	%f132, 0fBEFFFFFF;
	fma.rn.f32 	%f133, %f131, %f126, %f132;
	mov.f32 	%f134, 0f3F800000;
	fma.rn.f32 	%f135, %f133, %f126, %f134;
	fma.rn.f32 	%f137, %f126, %f154, %f149;
	mov.f32 	%f138, 0f3C0885E4;
	mov.f32 	%f139, 0fB94D4153;
	fma.rn.f32 	%f140, %f139, %f126, %f138;
	mov.f32 	%f141, 0fBE2AAAA8;
	fma.rn.f32 	%f142, %f140, %f126, %f141;
	fma.rn.f32 	%f143, %f142, %f137, %f154;
	and.b32  	%r86, %r99, 1;
	setp.eq.b32 	%p40, %r86, 1;
	selp.f32 	%f144, %f135, %f143, %p40;
	selp.f32 	%f145, %f143, %f135, %p40;
	and.b32  	%r87, %r99, 2;
	setp.eq.s32 	%p41, %r87, 0;
	neg.f32 	%f146, %f144;
	selp.f32 	%f147, %f144, %f146, %p41;
	add.s32 	%r88, %r99, 1;
	and.b32  	%r89, %r88, 2;
	setp.eq.s32 	%p42, %r89, 0;
	neg.f32 	%f148, %f145;
	selp.f32 	%f124, %f145, %f148, %p42;
	mad.lo.s32 	%r90, %r92, %r94, %r95;
	mad.lo.s32 	%r91, %r90, %r93, %r3;
	mul.wide.s32 	%rd21, %r91, 2;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.u32 	%r84, [%rd22];
	neg.f32 	%f125, %f147;
	// begin inline asm
	{ cvt.rn.f16x2.f32 %r82, %f125, %f124; }

	// end inline asm
	// begin inline asm
	{mul.f16x2 %r83,%r84,%r82;
}
	// end inline asm
	st.global.u32 	[%rd22], %r83;

$L__BB0_24:
	ret;

}

